{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5521,
     "status": "ok",
     "timestamp": 1721156796959,
     "user": {
      "displayName": "kms hamim",
      "userId": "17071813694682210730"
     },
     "user_tz": 240
    },
    "id": "kpqgR6NteBjC",
    "outputId": "c52fa4a6-8f79-428f-ecb9-4b725d7d652b"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XxIVU_gVdY9c"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import numba\n",
    "from numba import jit,njit, cuda\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score,precision_score, accuracy_score,average_precision_score,precision_recall_curve,auc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "import time\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import hypernetx as hnx\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and save Tox21 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert SMILES notation into graphs with initial featurizations for nodes and edges;\n",
    "#Load the dataset Tox21 with these graphs. \n",
    "start_time = time.time()\n",
    "smiles_to_g = SMILESToBigraph(node_featurizer=CanonicalAtomFeaturizer())\n",
    "dataset = Tox21(smiles_to_g)\n",
    "end_time = time.time()\n",
    "print(f'Load time = {end_time - start_time}')\n",
    "\n",
    "save_path = './saved_tox21_dataset.pt'\n",
    "torch.save(dataset, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved dataset\n",
    "start_time = time.time()\n",
    "load_path = './saved_tox21_dataset.pt'\n",
    "dataset = torch.load(load_path)\n",
    "end_time = time.time()\n",
    "print(f\"Dataset loaded from {load_path}\")\n",
    "print(f'Load time = {end_time - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract graphs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [graph for graph in dataset]\n",
    "label_data=[graph[2] for graph in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Hypergraphs from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the data rate limit for Jupyter Notebook\n",
    "!jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000\n",
    "# generate the jupyter notebook config file\n",
    "!jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dictionary file from scratch\n",
    "def process_text_to_dict(file_path):\n",
    "    # Initialize a defaultdict to store lists\n",
    "    result_dict = defaultdict(list)\n",
    "\n",
    "    # Read the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Process each line\n",
    "    for line in lines:\n",
    "        # Use regular expression to match the pattern (number,\"values\")\n",
    "        match = re.match(r'(\\d+),\"(.*)\"', line.strip())\n",
    "        if match:\n",
    "            # Extract the initial number and the comma-separated values\n",
    "            initial_number = int(match.group(1))\n",
    "            values = match.group(2).split(',')\n",
    "            # Add the initial number to the lists corresponding to each unique value\n",
    "            for value in set(values):\n",
    "                result_dict[int(value)].append(initial_number)\n",
    "        else:\n",
    "            # Check for lines without quotes but with only one value\n",
    "            match = re.match(r'(\\d+),(\\d+)', line.strip())\n",
    "            if match:\n",
    "                initial_number = int(match.group(1))\n",
    "                value = int(match.group(2))\n",
    "                result_dict[value].append(initial_number)\n",
    "                \n",
    "    # Convert defaultdict back to a regular dict before sorting\n",
    "    result_dict = dict(result_dict)\n",
    "\n",
    "    # Sort the dictionary by keys\n",
    "    sorted_dict = dict(sorted(result_dict.items()))\n",
    "\n",
    "    return sorted_dict\n",
    "\n",
    "# File path to the uploaded text file\n",
    "file_path = 'tox21_s1l2u7_info.txt'\n",
    "\n",
    "# Process the text file and get the dictionary\n",
    "result = process_text_to_dict(file_path)\n",
    "\n",
    "# Pickle the dictionary\n",
    "with open('result.pkl', 'wb') as file:\n",
    "    pickle.dump(result, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dictionary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZrvT9kx68wa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# when dict in pickle format\n",
    "file_path = 'result.pkl'\n",
    "with open(file_path, 'rb') as file:\n",
    "    DICT = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS29gH47Xrcp"
   },
   "outputs": [],
   "source": [
    "# when the dict in json format\n",
    "import json\n",
    "file_path = 'result.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    DICT = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Visualizing the hypergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "hypergraph_dict = data\n",
    "\n",
    "# Create the hypergraph\n",
    "H = hnx.Hypergraph(hypergraph_dict)\n",
    "\n",
    "# Save the hypergraph to a file\n",
    "with open('hypergraph.pkl', 'wb') as f:\n",
    "    pickle.dump(H, f)\n",
    "\n",
    "# Load the hypergraph from a file\n",
    "with open('hypergraph.pkl', 'rb') as f:\n",
    "    H = pickle.load(f)\n",
    "\n",
    "# Get the list of hyperedges\n",
    "hyperedges = list(H.edges())\n",
    "\n",
    "# Limit the number of hyperedges to be drawn\n",
    "max_hyperedges = 5  # Set the desired limit\n",
    "if len(hyperedges) > max_hyperedges:\n",
    "    sampled_hyperedges = random.sample(hyperedges, max_hyperedges)\n",
    "else:\n",
    "    sampled_hyperedges = hyperedges\n",
    "\n",
    "# Create a subset dictionary\n",
    "subset_hypergraph_dict = {k: H.edges[k] for k in sampled_hyperedges}\n",
    "\n",
    "# Create a new hypergraph using the subset\n",
    "H_subset = hnx.Hypergraph(subset_hypergraph_dict)\n",
    "\n",
    "# Draw the hypergraph\n",
    "hnx.draw(H_subset, layout=nx.spring_layout, layout_kwargs={'k': 0.3, 'iterations': 50}, node_radius=0.5, node_labels_kwargs={'fontsize': 3}, edge_labels_kwargs={'fontsize': 10}, with_node_labels=False, with_edge_labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbz8iyFefJWt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "converting to sparse matrix\n",
    "'''\n",
    "LEN=len(DICT)\n",
    "nl=coo_matrix((LEN, LEN))\n",
    "nl.setdiag(1)\n",
    "values = nl.data\n",
    "indices = np.vstack((nl.row, nl.col))\n",
    "i = torch.FloatTensor(indices)\n",
    "# v = torch.FloatTensor(values)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = nl.shape\n",
    "nl=torch.sparse_coo_tensor(i, v, torch.Size(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIKiv8wEKC_d"
   },
   "outputs": [],
   "source": [
    "LEN=len(DICT)\n",
    "nl=np.eye(LEN)\n",
    "nl=torch.from_numpy(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xiowNK_zKFrR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Then we are using the DICT to create a hyG. Drug=sequence=molecule=Hyperedge and subsequence=substructure=node\n",
    "'''\n",
    "molec_list = []\n",
    "chemicalsub_list = []\n",
    "molec_chemicalsub = {}\n",
    "#chemicalsub_drug = {}\n",
    "\n",
    "for molec in tqdm(DICT.keys(), desc='Loading dictionary') :\n",
    "    chemicalsubs = DICT[molec]\n",
    "    if molec not in molec_list :\n",
    "        molec_list.append(molec)\n",
    "    idx = molec_list.index(molec)\n",
    "    if idx not in molec_chemicalsub :\n",
    "        molec_chemicalsub[idx]=[]\n",
    "\n",
    "    translated_p = []\n",
    "\n",
    "    for chemicalsub in chemicalsubs:\n",
    "        if chemicalsub not in chemicalsub_list :\n",
    "            chemicalsub_list.append(chemicalsub)\n",
    "        p_idx = chemicalsub_list.index(chemicalsub)\n",
    "        translated_p.append(p_idx) # translate chemicalsub number into index\n",
    "\n",
    "    molec_chemicalsub[idx]=translated_p\n",
    "\n",
    "'''\n",
    "chemicalsub : corresponds to hypernode\n",
    "citing : corresponds to hyperedge(molec)\n",
    "'''\n",
    "\n",
    "chemicalsub_citing = [] # list of [chemicalsub molec]\n",
    "n_chemicalsub = len(chemicalsub_list)\n",
    "n_hedge = len(molec_list)\n",
    "\n",
    "for molec in tqdm(molec_chemicalsub.keys(), desc='Loading molecules'):\n",
    "    chemicalsubs = molec_chemicalsub[molec]\n",
    "    for chemicalsub in chemicalsubs :\n",
    "        chemicalsub_citing.append([chemicalsub, molec])\n",
    "\n",
    "chemicalsub_molec = torch.LongTensor(chemicalsub_citing)\n",
    "data_dict = {\n",
    "        ('node', 'in', 'edge'): (chemicalsub_molec[:,0], chemicalsub_molec[:,1]),\n",
    "        ('edge', 'con', 'node'): (chemicalsub_molec[:,1], chemicalsub_molec[:,0])\n",
    "    }\n",
    "\n",
    "'''\n",
    "finally passing the data_dict to construct hyG\n",
    "'''\n",
    "\n",
    "lst=[]\n",
    "for i in tqdm(chemicalsub_citing, desc='Constructing hypergraph'):\n",
    "  lst.append(i[0])\n",
    "s=set(lst)\n",
    "s=len(s)\n",
    "num_nodes_dict = {'edge': LEN,'node':s}\n",
    "hyG = dgl.heterograph(data_dict,num_nodes_dict=num_nodes_dict)\n",
    "rows=n_chemicalsub\n",
    "columns=n_hedge\n",
    "'''\n",
    "reading the feature (one hot coding) for molecs (edges)\n",
    "'''\n",
    "\n",
    "molec_X=nl\n",
    "v_feat=coo_matrix((rows, 128))\n",
    "v_feat.setdiag(1)\n",
    "#nl.toarray()\n",
    "values = v_feat.data\n",
    "indices = np.vstack((v_feat.row, v_feat.col))\n",
    "i = torch.FloatTensor(indices)\n",
    "# v = torch.FloatTensor(values)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = v_feat.shape\n",
    "v_feat=torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "\n",
    "hyG.ndata['h'] = {'edge' : molec_X.type('torch.FloatTensor'), 'node' : v_feat.type('torch.FloatTensor')}\n",
    "e_feat = molec_X.type('torch.FloatTensor')\n",
    "v_feat=v_feat.type('torch.FloatTensor')\n",
    "\n",
    "e_feat = e_feat.to(device)\n",
    "v_feat = v_feat.to(device)\n",
    "hyG=hyG.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply HyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lo9wd3hSRGD5"
   },
   "outputs": [],
   "source": [
    "class hypergraph_NN(nn.Module):\n",
    "    def __init__(self, i_d, q_d, v_d, e_d, num_class, dropout = 0.5):\n",
    "        super(hypergraph_NN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.q_d = q_d\n",
    "        self.first_layer_in = torch.nn.Linear(i_d, v_d)\n",
    "        self.not_first_layer_in = torch.nn.Linear(v_d, v_d)\n",
    "\n",
    "        self.w1 = torch.nn.Linear(e_d, q_d)\n",
    "        self.w2 = torch.nn.Linear(v_d, q_d)\n",
    "        self.w3 = torch.nn.Linear(v_d, e_d)\n",
    "        self.w4 = torch.nn.Linear(v_d, q_d)\n",
    "        self.w5 = torch.nn.Linear(e_d, q_d)\n",
    "        self.w6 = torch.nn.Linear(e_d, v_d)\n",
    "        self.cls = nn.Linear(e_d, num_class)\n",
    "    def red_function(self, nodes):\n",
    "        attention_score = F.softmax((nodes.mailbox['Attn']), dim=1)\n",
    "        aggreated = torch.sum(attention_score.unsqueeze(-1) * nodes.mailbox['v'], dim=1)\n",
    "        return {'h': aggreated}\n",
    "    def attention(self, edges):\n",
    "        attention_score = F.leaky_relu((edges.src['k'] * edges.dst['q']).sum(-1))\n",
    "        c=attention_score/np.sqrt(self.q_d)\n",
    "        return {'Attn': c}\n",
    "    def msg_function(self, edges):\n",
    "        return {'v': edges.src['v'], 'Attn': edges.data['Attn']}\n",
    "\n",
    "    def forward(self, hyG, vfeat, efeat,first_layer,last_layer):\n",
    "        with hyG.local_scope():\n",
    "            if first_layer:\n",
    "                feat_e = self.first_layer_in(efeat)\n",
    "            else:\n",
    "                feat_e = self.not_first_layer_in(efeat)\n",
    "            feat_v = vfeat\n",
    "\n",
    "            # Hyperedge attention\n",
    "            hyG.ndata['h'] = {'edge': feat_e}\n",
    "            hyG.ndata['k'] = {'edge' : self.w5(feat_e)}\n",
    "            hyG.ndata['v'] = {'edge' : self.w6(feat_e)}\n",
    "            hyG.ndata['q'] = {'node' : self.w4(feat_v)}\n",
    "            hyG.apply_edges(self.attention, etype='con')\n",
    "            hyG.update_all(self.msg_function, self.red_function, etype='con')\n",
    "            feat_v = hyG.ndata['h']['node']\n",
    "\n",
    "            # node attention\n",
    "            hyG.ndata['k'] = {'node' : self.w2(feat_v)}\n",
    "            hyG.ndata['v'] = {'node' : self.w3(feat_v)}\n",
    "            hyG.ndata['q'] = {'edge' : self.w1(feat_e)}\n",
    "            hyG.apply_edges(self.attention, etype='in')\n",
    "            hyG.update_all(self.msg_function, self.red_function, etype='in')\n",
    "            feat_e = hyG.ndata['h']['edge']\n",
    "\n",
    "\n",
    "            if not last_layer :\n",
    "                feat_v = F.dropout(feat_v, self.dropout)\n",
    "            if last_layer:\n",
    "                pred=self.cls(feat_e) #to reduce the hyperedge feature into 'c' dimension, where c is the number of class\n",
    "                return pred\n",
    "            else:\n",
    "                return [hyG, feat_v, feat_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATzNkpyD62Ub",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = torch.stack([label for label in label_data]).float()\n",
    "label=label_data[0].tolist()\n",
    "num_class=len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "file_path = 'hyGNN_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f'Test #{i + 1}:')\n",
    "    results = []\n",
    "    model =hypergraph_NN(molec_X.shape[1], 64, 128, 128,  num_class, 0.3)\n",
    "    model=model.to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    patience=0\n",
    "    \n",
    "    # Ensure the relevant features are of type Float\n",
    "    v_feat = v_feat.float()\n",
    "    e_feat = e_feat.float()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    min_loss = 1\n",
    "    max_loss = 0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc='Training progress'):\n",
    "        model.train()\n",
    "        train_meter = Meter()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pred = model(hyG, v_feat, e_feat, True,True)\n",
    "        \n",
    "        keys = list(range(7831))\n",
    "        f_pred = []\n",
    "        f_labels = []\n",
    "        \n",
    "        # Filter predictions and labels based on the presence of keys in DICT\n",
    "        for key, pred_item, label in zip(keys, pred, labels):\n",
    "            if str(key) in DICT:\n",
    "                f_pred.append(pred_item)\n",
    "                f_labels.append(label)\n",
    "\n",
    "        label=label_data[0].tolist()\n",
    "        num_class=len(label)\n",
    "    \n",
    "        f_labels = torch.stack([label for label in f_labels]).float()\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        train_label, test_label= train_test_split(f_labels,test_size=0.1, random_state=42)\n",
    "        train_label=torch.tensor(train_label,dtype=torch.float).to(device)\n",
    "        \n",
    "        size=int((LEN)*0.1)\n",
    "        val_label=train_label[0:size]\n",
    "        train_label=train_label[size:]\n",
    "        test_label=torch.tensor(test_label,dtype=torch.float).to(device)\n",
    "    \n",
    "        train_pred, test_pred= train_test_split(f_pred,test_size=0.1, random_state=42)\n",
    "        val_pred=train_pred[0:size]\n",
    "        train_pred=train_pred[size:]\n",
    "    \n",
    "        train_pred = torch.stack(train_pred)\n",
    "        val_pred = torch.stack(val_pred)\n",
    "        test_pred = torch.stack(test_pred)\n",
    "        \n",
    "         # Ensure the shapes match\n",
    "        assert train_pred.shape == train_label.shape, \"Prediction and label shapes do not match\"\n",
    "        \n",
    "        loss = loss_fn(train_pred, train_label)\n",
    "        # Convert logits to binary predictions using a threshold of 0.5\n",
    "        pred_cls = (train_pred > 0).float()\n",
    "        correct = torch.eq(pred_cls, train_label).sum().item()\n",
    "        total = train_label.numel()\n",
    "        train_acc = correct / total\n",
    "    \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_meter.update(train_pred, train_label)\n",
    "        print(f\"Loss: {loss.item():.4f}, Training Accuracy: {train_acc: .4f}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "          model.eval()\n",
    "          val_cls = (val_pred > 0).float()\n",
    "          correct = torch.eq(val_cls, val_label).sum().item()\n",
    "          total = val_label.numel()\n",
    "          val_acc = correct / total\n",
    "    \n",
    "          # Save the best validation accuracy and the corresponding test accuracy.\n",
    "          if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            E=i\n",
    "            patience=0\n",
    "            torch.save(model.state_dict(),'latest_hy.pth')\n",
    "          else:\n",
    "            patience+=1\n",
    "    \n",
    "        if patience==200:\n",
    "          break\n",
    "    \n",
    "        if loss < min_loss:\n",
    "                min_loss = loss\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "    \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "          print('Epoch {}/{}, train loss: {:.4f} (min {:.4f}), val_acc: {:.4f} (best_val_acc: {:.4f})'.format(epoch + 1, num_epochs, loss, min_loss, val_acc, best_val_acc))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "    training_time = f'{end_time - start_time:.2f}'\n",
    "    \n",
    "    # Measure the time for evaluation on the test set\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.load_state_dict(torch.load('latest_hy.pth'))\n",
    "    with torch.no_grad():\n",
    "      model.eval()\n",
    "      test_cls = (test_pred > 0).float()\n",
    "      correct = torch.eq(test_cls, test_label).sum().item()\n",
    "      total = test_label.numel()\n",
    "      test_acc = correct / total\n",
    "      \n",
    "    print(f\"Test Accuracy with HyGNN: {test_acc:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n",
    "    evaluation_time = f'{end_time - start_time:.2f}'\n",
    "    \n",
    "    hidden_size = 64\n",
    "    lr=0.01\n",
    "    results.append({\n",
    "            \"Model\": \"HyGNN\",\n",
    "            \"epoch\": num_epochs,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Test accuracy\": f'{test_acc:.4f}',\n",
    "            \"Best Val Acc.\": f'{best_val_acc:.4f}',\n",
    "            \"Training time (s)\": training_time,\n",
    "            \"Evaluation time (s)\": evaluation_time,\n",
    "            \"Lowest Loss\": f'{min_loss:.4f}',\n",
    "            \"Highest Loss\": f'{max_loss:.4f}'\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Append the DataFrame to the CSV file if it exists, otherwise create it\n",
    "    try:\n",
    "        # Try to read the existing file\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        # Append the new data\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    except FileNotFoundError:\n",
    "        # If the file does not exist, use the new data\n",
    "        updated_df = df\n",
    "    \n",
    "    # Save the updated DataFrame to the CSV file\n",
    "    updated_df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMExaBCX2nE4p5cHA4XFVeh",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (thtn)",
   "language": "python",
   "name": "thtn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
