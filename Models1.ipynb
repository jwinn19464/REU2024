{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33fe8b9-8ef7-49dc-a2f7-8f2fc04abc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.data.utils import split_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.data import Tox21\n",
    "from dgllife.utils import smiles_to_bigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "from dgl.nn.pytorch import GraphConv, GATConv, GATv2Conv, SAGEConv, GINConv\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
    "from rdkit import Chem\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "import concurrent.futures\n",
    "import time\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab67c84f-56b3-4800-8bbb-dd40993c40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda list --export > local_env.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "931cbcfb-7912-45f8-aea7-500b652789aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848fd494-ff04-413f-857c-505950dd768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 24.1 from C:\\Users\\jenif\\anaconda3\\envs\\py3.10.12\\lib\\site-packages\\pip (python 3.10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3014ff4-54d7-40fb-bc9b-12cf06f35164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jenif\\\\anaconda3\\\\envs\\\\py3.10.12\\\\lib\\\\site-packages\\\\rdkit\\\\__init__.py'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rdkit\n",
    "rdkit.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d327edfa-cd50-4b19-a119-b6c0bb40126c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AttachFileToLog',\n",
       " 'BlockLogs',\n",
       " 'DisableLog',\n",
       " 'EnableLog',\n",
       " 'LogDebugMsg',\n",
       " 'LogErrorMsg',\n",
       " 'LogInfoMsg',\n",
       " 'LogMessage',\n",
       " 'LogStatus',\n",
       " 'LogToCppStreams',\n",
       " 'LogToPythonLogger',\n",
       " 'LogToPythonStderr',\n",
       " 'LogWarningMsg',\n",
       " 'SeedRandomNumberGenerator',\n",
       " 'WrapLogs',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_iostreamsEnabled',\n",
       " '_listclass std::vector<int,class std::allocator<int> >',\n",
       " '_listclass std::vector<unsigned int,class std::allocator<unsigned int> >',\n",
       " '_listint',\n",
       " '_multithreadedEnabled',\n",
       " '_serializationEnabled',\n",
       " '_vectclass std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >',\n",
       " '_vectclass std::vector<double,class std::allocator<double> >',\n",
       " '_vectclass std::vector<int,class std::allocator<int> >',\n",
       " '_vectclass std::vector<unsigned int,class std::allocator<unsigned int> >',\n",
       " '_vectdouble',\n",
       " '_vectint',\n",
       " '_vectunsigned int',\n",
       " '_version',\n",
       " 'boostVersion',\n",
       " 'ostream',\n",
       " 'rdkitBuild',\n",
       " 'rdkitVersion',\n",
       " 'std_ostream',\n",
       " 'streambuf']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rdkit.rdBase\n",
    "dir(rdkit.rdBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f1e5e8-4eb3-4826-b303-b163f5380337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jenif\\\\anaconda3\\\\envs\\\\py3.10.12\\\\lib\\\\site-packages\\\\rdkit\\\\rdBase.pyd'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdkit.rdBase.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09410525-6c0e-4948-b680-22e91d4a1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dgl graphs from scratch...\n",
      "Processing molecule 1000/7831\n",
      "Processing molecule 2000/7831\n",
      "Processing molecule 3000/7831\n",
      "Processing molecule 4000/7831\n",
      "Processing molecule 5000/7831\n",
      "Processing molecule 6000/7831\n",
      "Processing molecule 7000/7831\n"
     ]
    }
   ],
   "source": [
    "# Use CanonicalAtomFeaturizer to generate node features\n",
    "node_featurizer = CanonicalAtomFeaturizer()\n",
    "edge_featurizer = CanonicalBondFeaturizer()\n",
    "\n",
    "def collate(samples):\n",
    "    graphs, labels = [], []\n",
    "    for sample in samples:\n",
    "        graphs.append(dgl.add_self_loop(sample[1]))  # Add self-loops during graph construction\n",
    "        labels.append(sample[2])  # Extract the labels (3rd element is the labels tensor)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    labels = torch.stack(labels)\n",
    "    return batched_graph, labels\n",
    "\n",
    "# Load the Tox21 dataset\n",
    "dataset = Tox21(smiles_to_graph=smiles_to_bigraph, node_featurizer=node_featurizer)\n",
    "# , edge_featurizer=edge_featurizer\n",
    "train_set, val_set, test_set = split_dataset(dataset, frac_list=[0.8, 0.1, 0.1], shuffle=True, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False, collate_fn=collate)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c5a423-5b03-4f55-b9c7-3943ef728df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\n",
    "hidden_size = 64\n",
    "num_epochs = 50\n",
    "checkpoint_dir = 'saveStates'\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bf1bfbf-8e6c-41c7-9ed0-a7823050e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, directory, filename):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    path = os.path.join(directory, filename)\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "\n",
    "# Function to load a checkpoint\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if os.path.isfile(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        return model, optimizer, epoch\n",
    "    return model, optimizer, -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bb73ec-0b89-4e5d-95d6-f8ea9e316689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = GraphConv(hidden_size, hidden_size)\n",
    "        self.classify = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float()\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5861f7bd-7d4c-4df1-a29f-12170e8d9a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Epoch 1/50, Loss: 0.2259, Validation Accuracy: 0.9297, (best 0.930)\n",
      "Epoch 2/50, Loss: 0.2060, Validation Accuracy: 0.9297, (best 0.930)\n",
      "Epoch 3/50, Loss: 0.2020, Validation Accuracy: 0.9294, (best 0.930)\n",
      "Epoch 4/50, Loss: 0.1987, Validation Accuracy: 0.9301, (best 0.930)\n",
      "Epoch 5/50, Loss: 0.1955, Validation Accuracy: 0.9297, (best 0.930)\n",
      "Epoch 6/50, Loss: 0.1947, Validation Accuracy: 0.9302, (best 0.930)\n",
      "Epoch 7/50, Loss: 0.1944, Validation Accuracy: 0.9303, (best 0.930)\n",
      "Epoch 8/50, Loss: 0.1912, Validation Accuracy: 0.9302, (best 0.930)\n",
      "Epoch 9/50, Loss: 0.1904, Validation Accuracy: 0.9316, (best 0.932)\n",
      "Epoch 10/50, Loss: 0.1886, Validation Accuracy: 0.9306, (best 0.932)\n",
      "Epoch 11/50, Loss: 0.1887, Validation Accuracy: 0.9304, (best 0.932)\n",
      "Epoch 12/50, Loss: 0.1871, Validation Accuracy: 0.9310, (best 0.932)\n",
      "Epoch 13/50, Loss: 0.1866, Validation Accuracy: 0.9294, (best 0.932)\n",
      "Epoch 14/50, Loss: 0.1853, Validation Accuracy: 0.9308, (best 0.932)\n",
      "Epoch 15/50, Loss: 0.1848, Validation Accuracy: 0.9299, (best 0.932)\n",
      "Epoch 16/50, Loss: 0.1852, Validation Accuracy: 0.9309, (best 0.932)\n",
      "Epoch 17/50, Loss: 0.1852, Validation Accuracy: 0.9305, (best 0.932)\n",
      "Epoch 18/50, Loss: 0.1838, Validation Accuracy: 0.9297, (best 0.932)\n",
      "Epoch 19/50, Loss: 0.1826, Validation Accuracy: 0.9305, (best 0.932)\n",
      "Epoch 20/50, Loss: 0.1819, Validation Accuracy: 0.9306, (best 0.932)\n",
      "Epoch 21/50, Loss: 0.1817, Validation Accuracy: 0.9308, (best 0.932)\n",
      "Epoch 22/50, Loss: 0.1815, Validation Accuracy: 0.9315, (best 0.932)\n",
      "Epoch 23/50, Loss: 0.1807, Validation Accuracy: 0.9317, (best 0.932)\n",
      "Epoch 24/50, Loss: 0.1794, Validation Accuracy: 0.9322, (best 0.932)\n",
      "Epoch 25/50, Loss: 0.1798, Validation Accuracy: 0.9294, (best 0.932)\n",
      "Epoch 26/50, Loss: 0.1799, Validation Accuracy: 0.9323, (best 0.932)\n",
      "Epoch 27/50, Loss: 0.1788, Validation Accuracy: 0.9321, (best 0.932)\n",
      "Epoch 28/50, Loss: 0.1794, Validation Accuracy: 0.9330, (best 0.933)\n",
      "Epoch 29/50, Loss: 0.1797, Validation Accuracy: 0.9309, (best 0.933)\n",
      "Epoch 30/50, Loss: 0.1789, Validation Accuracy: 0.9309, (best 0.933)\n",
      "Epoch 31/50, Loss: 0.1782, Validation Accuracy: 0.9327, (best 0.933)\n",
      "Epoch 32/50, Loss: 0.1775, Validation Accuracy: 0.9343, (best 0.934)\n",
      "Epoch 33/50, Loss: 0.1776, Validation Accuracy: 0.9320, (best 0.934)\n",
      "Epoch 34/50, Loss: 0.1780, Validation Accuracy: 0.9319, (best 0.934)\n",
      "Epoch 35/50, Loss: 0.1769, Validation Accuracy: 0.9303, (best 0.934)\n",
      "Epoch 36/50, Loss: 0.1766, Validation Accuracy: 0.9334, (best 0.934)\n",
      "Epoch 37/50, Loss: 0.1768, Validation Accuracy: 0.9327, (best 0.934)\n",
      "Epoch 38/50, Loss: 0.1777, Validation Accuracy: 0.9311, (best 0.934)\n",
      "Epoch 39/50, Loss: 0.1761, Validation Accuracy: 0.9322, (best 0.934)\n",
      "Epoch 40/50, Loss: 0.1764, Validation Accuracy: 0.9322, (best 0.934)\n",
      "Epoch 41/50, Loss: 0.1750, Validation Accuracy: 0.9317, (best 0.934)\n",
      "Epoch 42/50, Loss: 0.1758, Validation Accuracy: 0.9339, (best 0.934)\n",
      "Epoch 43/50, Loss: 0.1756, Validation Accuracy: 0.9324, (best 0.934)\n",
      "Epoch 44/50, Loss: 0.1758, Validation Accuracy: 0.9330, (best 0.934)\n",
      "Epoch 45/50, Loss: 0.1752, Validation Accuracy: 0.9322, (best 0.934)\n",
      "Epoch 46/50, Loss: 0.1753, Validation Accuracy: 0.9332, (best 0.934)\n",
      "Epoch 47/50, Loss: 0.1749, Validation Accuracy: 0.9304, (best 0.934)\n",
      "Epoch 48/50, Loss: 0.1747, Validation Accuracy: 0.9310, (best 0.934)\n",
      "Epoch 49/50, Loss: 0.1759, Validation Accuracy: 0.9317, (best 0.934)\n",
      "Epoch 50/50, Loss: 0.1747, Validation Accuracy: 0.9328, (best 0.934)\n",
      "Training time: 546.13 seconds\n",
      "Test Accuracy with GCN with parallel: 0.9360\n",
      "Evaluation time: 1.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GCN(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "best_val_acc = 0\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Save checkpoint at the end of each epoch\n",
    "        # save_checkpoint(model, optimizer, epoch, checkpoint_dir, 'saveState1.pth')\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "            \n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Check if a checkpoint exists and load it\n",
    "#model, optimizer, start_epoch = load_checkpoint(model, optimizer, os.path.join(checkpoint_dir, 'saveState1.pth'))\n",
    "#if start_epoch == -1:\n",
    "#    start_epoch = 0  # No checkpoint found, start from scratch\n",
    "#else:\n",
    "#    print(f\"Resuming from epoch {start_epoch + 1}\")\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}, (best {best_val_acc:.3f})\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GCN with parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1d1bb8-32e1-4b6a-9873-32e7ef39eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Epoch 1/50, Loss: 0.2245, Validation Accuracy: 0.9297\n",
      "Epoch 2/50, Loss: 0.2038, Validation Accuracy: 0.9297\n",
      "Epoch 3/50, Loss: 0.2024, Validation Accuracy: 0.9294\n",
      "Epoch 4/50, Loss: 0.1987, Validation Accuracy: 0.9304\n",
      "Epoch 5/50, Loss: 0.1955, Validation Accuracy: 0.9294\n",
      "Epoch 6/50, Loss: 0.1945, Validation Accuracy: 0.9315\n",
      "Epoch 7/50, Loss: 0.1908, Validation Accuracy: 0.9295\n",
      "Epoch 8/50, Loss: 0.1905, Validation Accuracy: 0.9322\n",
      "Epoch 9/50, Loss: 0.1906, Validation Accuracy: 0.9295\n",
      "Epoch 10/50, Loss: 0.1891, Validation Accuracy: 0.9305\n",
      "Epoch 11/50, Loss: 0.1878, Validation Accuracy: 0.9312\n",
      "Epoch 12/50, Loss: 0.1868, Validation Accuracy: 0.9319\n",
      "Epoch 13/50, Loss: 0.1868, Validation Accuracy: 0.9307\n",
      "Epoch 14/50, Loss: 0.1858, Validation Accuracy: 0.9319\n",
      "Epoch 15/50, Loss: 0.1841, Validation Accuracy: 0.9295\n",
      "Epoch 16/50, Loss: 0.1828, Validation Accuracy: 0.9317\n",
      "Epoch 17/50, Loss: 0.1839, Validation Accuracy: 0.9310\n",
      "Epoch 18/50, Loss: 0.1829, Validation Accuracy: 0.9317\n",
      "Epoch 19/50, Loss: 0.1823, Validation Accuracy: 0.9315\n",
      "Epoch 20/50, Loss: 0.1837, Validation Accuracy: 0.9309\n",
      "Epoch 21/50, Loss: 0.1810, Validation Accuracy: 0.9316\n",
      "Epoch 22/50, Loss: 0.1814, Validation Accuracy: 0.9322\n",
      "Epoch 23/50, Loss: 0.1821, Validation Accuracy: 0.9325\n",
      "Epoch 24/50, Loss: 0.1813, Validation Accuracy: 0.9307\n",
      "Epoch 25/50, Loss: 0.1807, Validation Accuracy: 0.9324\n",
      "Epoch 26/50, Loss: 0.1804, Validation Accuracy: 0.9326\n",
      "Epoch 27/50, Loss: 0.1797, Validation Accuracy: 0.9322\n",
      "Epoch 28/50, Loss: 0.1801, Validation Accuracy: 0.9311\n",
      "Epoch 29/50, Loss: 0.1798, Validation Accuracy: 0.9319\n",
      "Epoch 30/50, Loss: 0.1793, Validation Accuracy: 0.9314\n",
      "Epoch 31/50, Loss: 0.1790, Validation Accuracy: 0.9334\n",
      "Epoch 32/50, Loss: 0.1779, Validation Accuracy: 0.9323\n",
      "Epoch 33/50, Loss: 0.1772, Validation Accuracy: 0.9323\n",
      "Epoch 34/50, Loss: 0.1772, Validation Accuracy: 0.9340\n",
      "Epoch 35/50, Loss: 0.1774, Validation Accuracy: 0.9323\n",
      "Epoch 36/50, Loss: 0.1774, Validation Accuracy: 0.9328\n",
      "Epoch 37/50, Loss: 0.1770, Validation Accuracy: 0.9334\n",
      "Epoch 38/50, Loss: 0.1780, Validation Accuracy: 0.9350\n",
      "Epoch 39/50, Loss: 0.1774, Validation Accuracy: 0.9326\n",
      "Epoch 40/50, Loss: 0.1764, Validation Accuracy: 0.9319\n",
      "Epoch 41/50, Loss: 0.1764, Validation Accuracy: 0.9335\n",
      "Epoch 42/50, Loss: 0.1762, Validation Accuracy: 0.9318\n",
      "Epoch 43/50, Loss: 0.1759, Validation Accuracy: 0.9335\n",
      "Epoch 44/50, Loss: 0.1762, Validation Accuracy: 0.9337\n",
      "Epoch 45/50, Loss: 0.1768, Validation Accuracy: 0.9338\n",
      "Epoch 46/50, Loss: 0.1763, Validation Accuracy: 0.9319\n",
      "Epoch 47/50, Loss: 0.1749, Validation Accuracy: 0.9330\n",
      "Epoch 48/50, Loss: 0.1750, Validation Accuracy: 0.9340\n",
      "Epoch 49/50, Loss: 0.1741, Validation Accuracy: 0.9331\n",
      "Epoch 50/50, Loss: 0.1746, Validation Accuracy: 0.9326\n",
      "Training time: 545.55 seconds\n",
      "Test Accuracy with GCN w/o parallel: 0.9358\n",
      "Evaluation time: 1.02 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GCN(in_feats, hidden_size, num_classes)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GCN w/o parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0da820b9-5f08-493d-9a32-0de999a1cf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_feats, hidden_size, num_heads=2)\n",
    "        self.conv2 = GATConv(hidden_size * 2, hidden_size, num_heads=2)\n",
    "        self.classify = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float()\n",
    "        h = F.elu(self.conv1(g, h))\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of conv1\n",
    "        h = self.conv2(g, h)\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of conv2\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc08b807-2142-495e-a2d9-f6e34633e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655bad92-0c50-48c5-823f-7a35e5885985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Epoch 1/50, Loss: 0.2177, Validation Accuracy: 0.9297\n",
      "Epoch 2/50, Loss: 0.2037, Validation Accuracy: 0.9301\n",
      "Epoch 3/50, Loss: 0.2007, Validation Accuracy: 0.9298\n",
      "Epoch 4/50, Loss: 0.2013, Validation Accuracy: 0.9297\n",
      "Epoch 5/50, Loss: 0.1969, Validation Accuracy: 0.9308\n",
      "Epoch 6/50, Loss: 0.1981, Validation Accuracy: 0.9300\n",
      "Epoch 7/50, Loss: 0.1955, Validation Accuracy: 0.9302\n",
      "Epoch 8/50, Loss: 0.1943, Validation Accuracy: 0.9301\n",
      "Epoch 9/50, Loss: 0.1925, Validation Accuracy: 0.9317\n",
      "Epoch 10/50, Loss: 0.1915, Validation Accuracy: 0.9311\n",
      "Epoch 11/50, Loss: 0.1925, Validation Accuracy: 0.9323\n",
      "Epoch 12/50, Loss: 0.1926, Validation Accuracy: 0.9339\n",
      "Epoch 13/50, Loss: 0.1904, Validation Accuracy: 0.9304\n",
      "Epoch 14/50, Loss: 0.1903, Validation Accuracy: 0.9302\n",
      "Epoch 15/50, Loss: 0.1896, Validation Accuracy: 0.9331\n",
      "Epoch 16/50, Loss: 0.1893, Validation Accuracy: 0.9324\n",
      "Epoch 17/50, Loss: 0.1869, Validation Accuracy: 0.9325\n",
      "Epoch 18/50, Loss: 0.1859, Validation Accuracy: 0.9317\n",
      "Epoch 19/50, Loss: 0.1860, Validation Accuracy: 0.9302\n",
      "Epoch 20/50, Loss: 0.1875, Validation Accuracy: 0.9349\n",
      "Epoch 21/50, Loss: 0.1852, Validation Accuracy: 0.9317\n",
      "Epoch 22/50, Loss: 0.1865, Validation Accuracy: 0.9334\n",
      "Epoch 23/50, Loss: 0.1860, Validation Accuracy: 0.9331\n",
      "Epoch 24/50, Loss: 0.1853, Validation Accuracy: 0.9326\n",
      "Epoch 25/50, Loss: 0.1861, Validation Accuracy: 0.9319\n",
      "Epoch 26/50, Loss: 0.1853, Validation Accuracy: 0.9348\n",
      "Epoch 27/50, Loss: 0.1836, Validation Accuracy: 0.9328\n",
      "Epoch 28/50, Loss: 0.1850, Validation Accuracy: 0.9333\n",
      "Epoch 29/50, Loss: 0.1844, Validation Accuracy: 0.9331\n",
      "Epoch 30/50, Loss: 0.1839, Validation Accuracy: 0.9331\n",
      "Epoch 31/50, Loss: 0.1823, Validation Accuracy: 0.9317\n",
      "Epoch 32/50, Loss: 0.1853, Validation Accuracy: 0.9310\n",
      "Epoch 33/50, Loss: 0.1817, Validation Accuracy: 0.9328\n",
      "Epoch 34/50, Loss: 0.1828, Validation Accuracy: 0.9314\n",
      "Epoch 35/50, Loss: 0.1823, Validation Accuracy: 0.9309\n",
      "Epoch 36/50, Loss: 0.1817, Validation Accuracy: 0.9316\n",
      "Epoch 37/50, Loss: 0.1827, Validation Accuracy: 0.9328\n",
      "Epoch 38/50, Loss: 0.1810, Validation Accuracy: 0.9316\n",
      "Epoch 39/50, Loss: 0.1818, Validation Accuracy: 0.9353\n",
      "Epoch 40/50, Loss: 0.1822, Validation Accuracy: 0.9334\n",
      "Epoch 41/50, Loss: 0.1830, Validation Accuracy: 0.9327\n",
      "Epoch 42/50, Loss: 0.1800, Validation Accuracy: 0.9335\n",
      "Epoch 43/50, Loss: 0.1818, Validation Accuracy: 0.9323\n",
      "Epoch 44/50, Loss: 0.1806, Validation Accuracy: 0.9326\n",
      "Epoch 45/50, Loss: 0.1796, Validation Accuracy: 0.9317\n",
      "Epoch 46/50, Loss: 0.1794, Validation Accuracy: 0.9327\n",
      "Epoch 47/50, Loss: 0.1811, Validation Accuracy: 0.9321\n",
      "Epoch 48/50, Loss: 0.1797, Validation Accuracy: 0.9314\n",
      "Epoch 49/50, Loss: 0.1789, Validation Accuracy: 0.9322\n",
      "Epoch 50/50, Loss: 0.1793, Validation Accuracy: 0.9347\n",
      "Training time: 644.76 seconds\n",
      "Test Accuracy with GAT w/parallel: 0.9371\n",
      "Evaluation time: 1.12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GAT(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GAT w/parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91bca7b-f8de-4b3b-8981-10c3d8885d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h': tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Epoch 1/50, Loss: 0.2162, Validation Accuracy: 0.9299\n",
      "Epoch 2/50, Loss: 0.2040, Validation Accuracy: 0.9300\n",
      "Epoch 3/50, Loss: 0.2003, Validation Accuracy: 0.9295\n",
      "Epoch 4/50, Loss: 0.1981, Validation Accuracy: 0.9317\n",
      "Epoch 5/50, Loss: 0.1962, Validation Accuracy: 0.9301\n",
      "Epoch 6/50, Loss: 0.1952, Validation Accuracy: 0.9311\n",
      "Epoch 7/50, Loss: 0.1923, Validation Accuracy: 0.9300\n",
      "Epoch 8/50, Loss: 0.1937, Validation Accuracy: 0.9295\n",
      "Epoch 9/50, Loss: 0.1935, Validation Accuracy: 0.9323\n",
      "Epoch 10/50, Loss: 0.1931, Validation Accuracy: 0.9321\n",
      "Epoch 11/50, Loss: 0.1901, Validation Accuracy: 0.9310\n",
      "Epoch 12/50, Loss: 0.1906, Validation Accuracy: 0.9304\n",
      "Epoch 13/50, Loss: 0.1898, Validation Accuracy: 0.9327\n",
      "Epoch 14/50, Loss: 0.1906, Validation Accuracy: 0.9316\n",
      "Epoch 15/50, Loss: 0.1896, Validation Accuracy: 0.9308\n",
      "Epoch 16/50, Loss: 0.1893, Validation Accuracy: 0.9308\n",
      "Epoch 17/50, Loss: 0.1892, Validation Accuracy: 0.9304\n",
      "Epoch 18/50, Loss: 0.1877, Validation Accuracy: 0.9311\n",
      "Epoch 19/50, Loss: 0.1877, Validation Accuracy: 0.9311\n",
      "Epoch 20/50, Loss: 0.1877, Validation Accuracy: 0.9303\n",
      "Epoch 21/50, Loss: 0.1867, Validation Accuracy: 0.9310\n",
      "Epoch 22/50, Loss: 0.1859, Validation Accuracy: 0.9317\n",
      "Epoch 23/50, Loss: 0.1866, Validation Accuracy: 0.9305\n",
      "Epoch 24/50, Loss: 0.1872, Validation Accuracy: 0.9307\n",
      "Epoch 25/50, Loss: 0.1839, Validation Accuracy: 0.9307\n",
      "Epoch 26/50, Loss: 0.1843, Validation Accuracy: 0.9310\n",
      "Epoch 27/50, Loss: 0.1854, Validation Accuracy: 0.9316\n",
      "Epoch 28/50, Loss: 0.1855, Validation Accuracy: 0.9311\n",
      "Epoch 29/50, Loss: 0.1843, Validation Accuracy: 0.9310\n",
      "Epoch 30/50, Loss: 0.1845, Validation Accuracy: 0.9321\n",
      "Epoch 31/50, Loss: 0.1840, Validation Accuracy: 0.9304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GAT(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "        \n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        # Debugging statement\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GAT w/o parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5a8be-7041-451c-a807-4eb1e3b257fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GATv2, self).__init__()\n",
    "        self.conv1 = GATv2Conv(in_feats, hidden_size, num_heads=2)\n",
    "        self.conv2 = GATv2Conv(hidden_size * 2, hidden_size, num_heads=2)\n",
    "        self.classify = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float()\n",
    "        h = F.elu(self.conv1(g, h))\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of conv1\n",
    "        h = self.conv2(g, h)\n",
    "        h = h.view(h.size(0), -1)  # Flatten the output of conv2\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e69daf-1d16-43ba-afbe-f8fb4412ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GATv2(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        # Debugging statement\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GATv2 with parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c641f-59de-499c-8ef0-bb32b466e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GATv2(in_feats, hidden_size, num_classes)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        # Debugging statement\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GATv2 w/o parallel: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ec7f2-3d66-4cf6-910b-9ff815b99bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7523d0-9387-43a6-8d71-177e5ef90852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes, aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, hidden_size, aggregator_type)\n",
    "        self.conv2 = SAGEConv(hidden_size, hidden_size, aggregator_type)\n",
    "        self.classify = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float()\n",
    "        h = F.elu(self.conv1(g, h))\n",
    "        #h = h.view(h.size(0), -1)  # Flatten the output of conv1\n",
    "        h = self.conv2(g, h)\n",
    "        #h = h.view(h.size(0), -1)  # Flatten the output of conv2\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h')\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5d17f-4295-4cd6-a46d-6448ab137527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        # Debugging statement\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c309d-429a-439f-9693-dfa48f498651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "aggregator_type = 'mean'\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GraphSAGE(in_feats, hidden_size, num_classes, aggregator_type)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GraphSAGE with mean aggregation: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f80dd-56e3-4596-b8f9-3d6ab36d705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 64\n",
    "aggregator_type = 'pool'\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GraphSAGE(in_feats, hidden_size, num_classes, aggregator_type)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GraphSAGE with pool aggregation: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e92573-a22b-4300-90e3-c6be353be002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 64\n",
    "aggregator_type = 'lstm'\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GraphSAGE(in_feats, hidden_size, num_classes, aggregator_type)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GraphSAGE with lstm aggregation: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09195b-9dff-4c28-b2ae-9d91ae45abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 64\n",
    "aggregator_type = 'gcn'\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GraphSAGE(in_feats, hidden_size, num_classes, aggregator_type)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GraphSAGE with gcn aggregation: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c422cd2-3194-4151-9f15-c10f1be01c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GINConv\n",
    "# classdgl.nn.pytorch.conv.GINConv(apply_func=None, aggregator_type='sum', init_eps=0, learn_eps=False, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29ad28-7d15-41f6-8482-715afa3a4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(in_feats, hidden_size),\n",
    "                       BatchNorm1d(hidden_size), ReLU(),\n",
    "                       Linear(hidden_size, hidden_size), ReLU()))\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(Linear(hidden_size, hidden_size), BatchNorm1d(hidden_size), ReLU(),\n",
    "                       Linear(hidden_size, hidden_size), ReLU()))\n",
    "\n",
    "        # self.conv3 = GINConv(\n",
    "        #     Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
    "        #                Linear(dim_h, dim_h), ReLU()))\n",
    "        \n",
    "        #self.lin1 = Linear(dim_h*3, dim_h*3)\n",
    "        self.lin1 = Linear(hidden_size*2, num_classes)\n",
    "        self.classify = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['h'].float()\n",
    "        h = F.relu(self.conv1(g, h))\n",
    "        h = self.conv2(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            hg = dgl.mean_nodes(g, 'h') # try sum_nodes\n",
    "        return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb525ac0-4aeb-4738-96df-d6d1600ee5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first graph to get the feature dimension\n",
    "first_graph = dataset[0][1]\n",
    "print(first_graph.ndata)\n",
    "in_feats = first_graph.ndata['h'].shape[1]\n",
    "# hidden_size = 16\n",
    "\n",
    "# Determine the number of tasks\n",
    "num_classes = dataset.labels.shape[1]\n",
    "model = GIN(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Use DataParallel to wrap the model for parallel training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batched_graph, labels in dataloader:\n",
    "\n",
    "        batched_graph = batched_graph.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model(batched_graph)\n",
    "        # Debugging statement\n",
    "        loss = criterion(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batched_graph, labels in dataloader:\n",
    "\n",
    "            batched_graph = batched_graph.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logits = model(batched_graph)\n",
    "            preds = (logits > 0.5).float()\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Measure the time for training\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "# num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, criterion, optimizer)\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Measure the time for evaluation on the test set\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_acc = evaluate(model, test_loader)\n",
    "print(f\"Test Accuracy with GIN: {test_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Evaluation time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe400ef3-bab4-44fa-b365-a8adb6b0e3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59954146-15b2-4208-943e-c067d21562c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore GIN with edge features\n",
    "# GINEConv\n",
    "# classdgl.nn.pytorch.conv.GINEConv(apply_func=None, init_eps=0, learn_eps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91adcb-243f-4a5b-8527-36c4e8d5cee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec356db-f803-40e7-969c-266a99bbcccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tox21_recent import moltree_to_dglgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda851a-1b47-4162-8a71-ab10abd0c97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843f51-c7d2-41f1-8546-0818cb6d3a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62cd42-9e4a-4e25-8833-e8ca39e287d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162777e-0c98-4535-b904-4f05c1ab9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8eaa8a-1e63-4e82-9c16-774cd6fbd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_weighted_graph(smiles):\n",
    "    for graph in train_set:\n",
    "        smiles = graph[0]\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        # print(smiles)\n",
    "    \n",
    "    print(smiles_to_bigraph(smiles, node_featurizer=featurize_atoms, edge_featurizer=featurize_bonds))\n",
    "\n",
    "    # Generate the molecule's adjacency matrix with bond order \n",
    "    G = Chem.GetAdjacencyMatrix(mol, useBO=True)\n",
    "    \n",
    "    # Convert the weighted adjacency matrix to a NumPy array\n",
    "    adjacency_matrix = np.asarray(G)\n",
    "    \n",
    "    # Print the weighted adjacency matrix\n",
    "    print(adjacency_matrix)\n",
    "    \n",
    "    # Convert weighted adjacency matrix to NetworkX graph\n",
    "    G = nx.from_numpy_array(adjacency_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f0ee72-2505-4d60-8782-cdef5ed1350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader takes away SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570ca6a-d919-470a-b0b9-c7df55999df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c813ce-bf1d-4811-99cf-df669a82df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8253595-c859-4146-85f6-91e9115f7679",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1bb6c-2a85-43e5-a5ed-3cc2cda4a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph in train_set:\n",
    "    smiles = graph[0]\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # print(smiles)\n",
    "    \n",
    "    graph = smiles_to_bigraph(smiles, node_featurizer=featurize_atoms, edge_featurizer=featurize_bonds)\n",
    "    dir(graph)\n",
    "    # Generate the molecule's adjacency matrix with bond order \n",
    "    # G = Chem.GetAdjacencyMatrix(mol, useBO=True)\n",
    "    \n",
    "    # # Convert the weighted adjacency matrix to a NumPy array\n",
    "    # adjacency_matrix = np.asarray(G)\n",
    "    \n",
    "    # # Print the weighted adjacency matrix\n",
    "    # print(adjacency_matrix)\n",
    "    \n",
    "    # # Convert weighted adjacency matrix to NetworkX graph\n",
    "    # G = nx.from_numpy_array(adjacency_matrix)\n",
    "    # print(G)\n",
    "    # print(type(G))\n",
    "    # G = dgl.from_networkx(G)\n",
    "    # print(G)\n",
    "    # print(type(G))\n",
    "    # #nx.draw(G, with_labels=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57429c6-bc62-4311-9be5-79ee97789b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = train_set[0][0]\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "# graph = smiles_to_bigraph(smiles, node_featurizer=featurize_atoms, edge_featurizer=featurize_bonds)\n",
    "\n",
    "node_featurizer = CanonicalAtomFeaturizer()\n",
    "edge_featurizer = CanonicalBondFeaturizer()\n",
    "graph = smiles_to_bigraph(smiles, node_featurizer=node_featurizer, edge_featurizer=edge_featurizer)\n",
    "\n",
    "print(graph.adjacency_matrix())\n",
    "print(dir(graph))\n",
    "# Generate the molecule's adjacency matrix with bond order \n",
    "# G = Chem.GetAdjacencyMatrix(mol, useBO=True)\n",
    "\n",
    "# # Convert the weighted adjacency matrix to a NumPy array\n",
    "# adjacency_matrix = np.asarray(G)\n",
    "\n",
    "# # Print the weighted adjacency matrix\n",
    "# print(adjacency_matrix)\n",
    "    \n",
    "    # # Convert weighted adjacency matrix to NetworkX graph\n",
    "    # G = nx.from_numpy_array(adjacency_matrix)\n",
    "    # print(G)\n",
    "    # print(type(G))\n",
    "    # G = dgl.from_networkx(G)\n",
    "    # print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a0f8c-2275-4298-8399-5c6155f0c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185f73a-8ba4-4725-9620-769a00a1c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Edge Features (Bond Order):\")\n",
    "print(graph.edata['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5a19a-5c0c-4e19-81e0-aab5c5a22386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the bond orders match RDKit adjacency matrix with bond order\n",
    "adj_matrix_rdkit = Chem.GetAdjacencyMatrix(mol, useBO=True)\n",
    "print(\"RDKit Adjacency Matrix with Bond Order:\")\n",
    "print(np.asarray(adj_matrix_rdkit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a18b5b-d357-4f00-95a3-3c248f7a8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize bonds and create the graph\n",
    "smiles = train_set[0][0]\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "graph = smiles_to_bigraph(smiles, node_featurizer=featurize_atoms, edge_featurizer=featurize_bonds)\n",
    "\n",
    "# Print edge features\n",
    "print(graph.edata['type'])\n",
    "\n",
    "# Print the adjacency matrix from DGL\n",
    "print(graph.adjacency_matrix())\n",
    "\n",
    "# Generate the molecule's adjacency matrix with bond order from RDKit\n",
    "G = Chem.GetAdjacencyMatrix(mol, useBO=True)\n",
    "\n",
    "# Convert the weighted adjacency matrix to a NumPy array\n",
    "adjacency_matrix = np.asarray(G)\n",
    "\n",
    "# Print the weighted adjacency matrix\n",
    "print(adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e3c0e7-a491-4d40-8a89-cdca8ef5dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84af7b-4c7b-454e-a999-7ad5c1e7dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ece88f-1d45-4305-8748-f8ce3d473a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07b8bc-cde7-4db2-8e86-b0b691b58e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.ntypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f2d99-0c51-4040-886a-b127b856d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.dgl_sparse\n",
    "dgl.__version__\n",
    "help(dgl.dgl_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e461c393-6bc0-448b-a150-879f75de067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dgl_sparse import spmatrix\n",
    "dir(spmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cf0c5-51ca-4776-acd6-e5926c734259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc7623-01bb-40db-aab3-61aee4e205f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f259d73-3a8e-49a6-892e-970ed64653ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(dgl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6205e3b3-beaf-495e-be2a-1709a1e45955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version that weights graph by index of bond types (Anh's version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5d1b1-0ba1-43b2-95d1-0643e3a0221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_atoms(mol):\n",
    "    feats = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feats.append(atom.GetAtomicNum())\n",
    "    return {'atomic': torch.tensor(feats).reshape(-1,1).float()}\n",
    "\n",
    "def featurize_bonds(mol):\n",
    "    feats = []\n",
    "    bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,  #enumeration value \n",
    "                  Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    for bond in mol.GetBonds():\n",
    "        btype = bond_types.index(bond.GetBondType())\n",
    "        # One bond between atom u and v corresponds to two edges (u, v) and (v, u). Dataset[0][1] has 17 bonds and 34 edges. \n",
    "        feats.extend([btype, btype])\n",
    "        # print('bond type:', int(bond.GetBondType()))\n",
    "        # print('b type: ', btype)\n",
    "    return {'type': torch.tensor(feats).reshape(-1, 1).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71464c2-f805-4e10-8724-095a67d9db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version that weights graph by bond type integers (test version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3070c1-0bbb-45f7-9598-823411782a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_bondsm(mol):\n",
    "    feats = []\n",
    "    bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,  #enumeration value \n",
    "                  Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    for bond in mol.GetBonds():\n",
    "        btype = int(bond.GetBondType())\n",
    "        # One bond between atom u and v corresponds to two edges (u, v) and (v, u). Dataset[0][1] has 17 bonds and 34 edges. \n",
    "        feats.extend([btype, btype])\n",
    "    return {'type': torch.tensor(feats).reshape(-1, 1).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761cdbb-9875-488e-a32c-aa6456eff58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurize_bonds(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214342a0-4c89-42bb-bd4b-83341542b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
